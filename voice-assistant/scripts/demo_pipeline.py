#!/usr/bin/env python3
"""
Voice Assistant Pipeline Demo
Demonstrates the full pipeline: ASR â†’ Translation â†’ Intent â†’ LLM â†’ Translation â†’ TTS
"""

import sys
import os
from pathlib import Path

# Add src to path
sys.path.insert(0, str(Path(__file__).parent.parent / "src"))

import logging

# Setup logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


def print_banner():
    """Print demo banner."""
    print("\n" + "=" * 70)
    print("   ğŸ™ï¸  HYPER-LOCALIZED MULTILINGUAL VOICE ASSISTANT")
    print("   ğŸ“ Malayalam â†’ English â†’ LLM â†’ English â†’ Malayalam")
    print("=" * 70)


def print_result(result):
    """Pretty print pipeline result."""
    print("\n" + "-" * 70)
    print("ğŸ“Š PIPELINE RESULTS")
    print("-" * 70)

    # Input
    print(f"\nğŸ“¥ INPUT:")
    print(f"   Malayalam: {result.malayalam_text}")

    # Translation
    print(f"\nğŸ”„ TRANSLATION (ML â†’ EN):")
    print(f"   English: {result.english_text}")
    print(f"   â±ï¸  Time: {result.translation_ml_en_time_ms:.1f}ms")

    # Intent Detection
    if result.intent_type:
        print(f"\nğŸ¯ INTENT DETECTED:")
        print(f"   Type: {result.intent_type.upper()}")
        print(f"   Description: {result.intent_description}")
        print(f"   Confidence: {result.intent_confidence:.0%}")
        if result.intent_entities:
            print(f"   Entities: {result.intent_entities}")

    # LLM Response
    print(f"\nğŸ¤– LLM RESPONSE:")
    print(f"   English: {result.english_response}")
    print(f"   â±ï¸  Time: {result.llm_time_ms:.1f}ms")

    # Translation back
    print(f"\nğŸ”„ TRANSLATION (EN â†’ ML):")
    print(f"   Malayalam: {result.malayalam_response}")
    print(f"   â±ï¸  Time: {result.translation_en_ml_time_ms:.1f}ms")

    # TTS
    print(f"\nğŸ”Š TTS:")
    print(f"   â±ï¸  Time: {result.tts_time_ms:.1f}ms")

    # Total
    print(f"\nâ±ï¸  TOTAL TIME: {result.total_time_ms:.1f}ms ({result.total_time_ms/1000:.2f}s)")

    if not result.success:
        print(f"\nâŒ ERROR: {result.error}")

    print("-" * 70)


def demo_text_input():
    """Demo with text input (no audio recording needed)."""
    from pipeline import VoiceAssistantPipeline

    print_banner()

    # Test inputs in Malayalam
    test_inputs = [
        # Greeting
        ("à´¨à´®à´¸àµà´•à´¾à´°à´‚, à´à´¨àµà´±àµ† à´ªàµ‡à´°àµ à´°à´¾à´¹àµàµ½ à´†à´£àµ", "Greeting - Introduction"),
        # Weather question
        ("à´‡à´¨àµà´¨à´¤àµà´¤àµ† à´•à´¾à´²à´¾à´µà´¸àµà´¥ à´à´™àµà´™à´¨àµ†à´¯àµà´£àµà´Ÿàµ?", "Weather - Question"),
        # Time question
        ("à´‡à´ªàµà´ªàµ‹àµ¾ à´¸à´®à´¯à´‚ à´à´¤àµà´°à´¯à´¾à´¯à´¿?", "Time - Question"),
        # General question
        ("à´•àµ‡à´°à´³à´¤àµà´¤à´¿à´¨àµà´±àµ† à´¤à´²à´¸àµà´¥à´¾à´¨à´‚ à´à´¤à´¾à´£àµ?", "Information - Question"),
        # Task/Command
        ("à´à´¨à´¿à´•àµà´•àµ à´’à´°àµ à´±à´¿à´®àµˆàµ»à´¡àµ¼ à´¸àµ†à´±àµà´±àµ à´šàµ†à´¯àµà´¯à´£à´‚", "Task - Reminder"),
    ]

    # Initialize pipeline
    print("\nğŸ”§ Initializing pipeline...")
    pipeline = VoiceAssistantPipeline(
        asr_engine="whisper",
        asr_model_size="base",
        llm_model_size="1.5b",
        tts_engine="mms",
        device="cuda",
        detect_intent=True,
    )

    # Load components
    pipeline.load_components(show_progress=True)

    # Output directory
    output_dir = Path(__file__).parent.parent / "demo_outputs"
    output_dir.mkdir(exist_ok=True)

    # Process each test input
    for i, (malayalam_text, description) in enumerate(test_inputs, 1):
        print(f"\n\n{'='*70}")
        print(f"ğŸ“ TEST {i}: {description}")
        print(f"{'='*70}")

        output_audio = output_dir / f"response_{i}.wav"

        result = pipeline.process_text(
            text=malayalam_text,
            input_language="ml",
            output_audio_path=str(output_audio),
        )

        print_result(result)

        if result.success:
            print(f"ğŸ’¾ Audio saved to: {output_audio}")

        # Clear conversation for next test
        pipeline.clear_conversation()

    print(f"\n\n{'='*70}")
    print("âœ… DEMO COMPLETE!")
    print(f"ğŸ“ Output files saved to: {output_dir}")
    print(f"{'='*70}\n")


def demo_interactive():
    """Interactive demo - type Malayalam text and get responses."""
    from pipeline import VoiceAssistantPipeline

    print_banner()
    print("\nğŸ® INTERACTIVE MODE")
    print("Type Malayalam text and press Enter. Type 'quit' to exit.\n")

    # Initialize pipeline
    print("ğŸ”§ Initializing pipeline...")
    pipeline = VoiceAssistantPipeline(
        asr_engine="whisper",
        llm_model_size="1.5b",
        tts_engine="mms",
        device="cuda",
        detect_intent=True,
    )
    pipeline.load_components(show_progress=True)

    output_dir = Path(__file__).parent.parent / "demo_outputs"
    output_dir.mkdir(exist_ok=True)

    response_count = 0

    while True:
        try:
            print("\n" + "-" * 50)
            user_input = input("ğŸ‘¤ You (Malayalam): ").strip()

            if user_input.lower() in ["quit", "exit", "q"]:
                break

            if not user_input:
                continue

            response_count += 1
            output_audio = output_dir / f"interactive_{response_count}.wav"

            result = pipeline.process_text(
                text=user_input,
                input_language="ml",
                output_audio_path=str(output_audio),
            )

            print(f"\nğŸ”„ English: {result.english_text}")

            if result.intent_type:
                print(f"ğŸ¯ Intent: {result.intent_type.upper()} ({result.intent_confidence:.0%})")
                print(f"   â†’ {result.intent_description}")

            print(f"\nğŸ¤– Response (EN): {result.english_response}")
            print(f"ğŸ—£ï¸  Response (ML): {result.malayalam_response}")
            print(f"\nâ±ï¸  Total: {result.total_time_ms:.1f}ms")
            print(f"ğŸ’¾ Audio: {output_audio}")

        except KeyboardInterrupt:
            break

    print("\n\nâœ… Demo ended. Goodbye!")


def demo_with_audio():
    """Demo with audio file input."""
    from pipeline import VoiceAssistantPipeline

    print_banner()

    # Check for audio files in test directory
    test_audio_dir = Path(__file__).parent.parent / "test_audio"

    if not test_audio_dir.exists():
        print(f"\nâš ï¸  No test audio directory found at: {test_audio_dir}")
        print("Creating directory. Please add Malayalam audio files (.wav) and run again.")
        test_audio_dir.mkdir(exist_ok=True)
        return

    audio_files = list(test_audio_dir.glob("*.wav"))

    if not audio_files:
        print(f"\nâš ï¸  No .wav files found in: {test_audio_dir}")
        print("Please add Malayalam audio files and run again.")
        return

    print(f"\nğŸ“ Found {len(audio_files)} audio file(s)")

    # Initialize pipeline
    print("\nğŸ”§ Initializing pipeline...")
    pipeline = VoiceAssistantPipeline(
        asr_engine="whisper",
        asr_model_size="base",
        llm_model_size="1.5b",
        tts_engine="mms",
        device="cuda",
        detect_intent=True,
    )
    pipeline.load_components(show_progress=True)

    output_dir = Path(__file__).parent.parent / "demo_outputs"
    output_dir.mkdir(exist_ok=True)

    # Process each audio file
    for i, audio_file in enumerate(audio_files, 1):
        print(f"\n\n{'='*70}")
        print(f"ğŸ™ï¸  Processing: {audio_file.name}")
        print(f"{'='*70}")

        output_audio = output_dir / f"response_{audio_file.stem}.wav"

        result = pipeline.process(
            audio_path=str(audio_file),
            output_audio_path=str(output_audio),
        )

        print_result(result)

        if result.success:
            print(f"ğŸ’¾ Audio saved to: {output_audio}")

        pipeline.clear_conversation()

    print(f"\n\n{'='*70}")
    print("âœ… DEMO COMPLETE!")
    print(f"{'='*70}\n")


def main():
    """Main entry point."""
    import argparse

    parser = argparse.ArgumentParser(description="Voice Assistant Pipeline Demo")
    parser.add_argument(
        "--mode",
        choices=["text", "interactive", "audio"],
        default="text",
        help="Demo mode: text (predefined), interactive, or audio (from files)"
    )

    args = parser.parse_args()

    if args.mode == "text":
        demo_text_input()
    elif args.mode == "interactive":
        demo_interactive()
    elif args.mode == "audio":
        demo_with_audio()


if __name__ == "__main__":
    main()
